{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9ff8439",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks: A Visual and Mathematical Journey\n",
    "\n",
    "Welcome to this comprehensive tutorial on Convolutional Neural Networks (CNNs)! This notebook is designed for graduate students in data science with backgrounds in mathematics, computer science, and statistics.\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this tutorial, you will understand:\n",
    "- The mathematical foundations of convolution operations\n",
    "- How CNN layers work and their parameters\n",
    "- Backpropagation in convolutional layers  \n",
    "- Complete CNN architectures and their design principles\n",
    "- How to visualize and interpret what CNNs learn\n",
    "- Practical implementation using PyTorch\n",
    "\n",
    "## Table of Contents\n",
    "1. [Import Required Libraries](#1.-Import-Required-Libraries)\n",
    "2. [Understanding Convolution Operation](#2.-Understanding-Convolution-Operation)\n",
    "3. [Building Convolutional Layers](#3.-Building-Convolutional-Layers)\n",
    "4. [Implementing Pooling Operations](#4.-Implementing-Pooling-Operations)\n",
    "5. [Visualizing Feature Maps](#5.-Visualizing-Feature-Maps)\n",
    "6. [Constructing a Complete CNN Architecture](#6.-Constructing-a-Complete-CNN-Architecture)\n",
    "7. [Forward Pass Implementation](#7.-Forward-Pass-Implementation)\n",
    "8. [Backpropagation in CNNs](#8.-Backpropagation-in-CNNs)\n",
    "9. [Training Loop with Real Data](#9.-Training-Loop-with-Real-Data)\n",
    "10. [Visualizing Learned Filters](#10.-Visualizing-Learned-Filters)\n",
    "11. [Comparing Different CNN Architectures](#11.-Comparing-Different-CNN-Architectures)\n",
    "\n",
    "Let's begin our journey!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b94f06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695783a5",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "We'll start by importing all the essential libraries for our CNN tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a72468d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸  Torchvision compatibility issue: partially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)\n",
      "Installing compatible versions...\n",
      "âœ“ Using device: cpu\n",
      "âœ“ All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Essential libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Handle torchvision compatibility issues\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "try:\n",
    "    import torchvision\n",
    "    import torchvision.transforms as transforms\n",
    "    # Test torchvision functionality\n",
    "    _ = transforms.ToTensor()\n",
    "    print(f\"âœ“ PyTorch version: {torch.__version__}\")\n",
    "    print(f\"âœ“ Torchvision version: {torchvision.__version__}\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  Torchvision compatibility issue: {e}\")\n",
    "    print(\"Installing compatible versions...\")\n",
    "    # Fallback: we'll implement basic transforms manually if needed\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib.patches import Rectangle\n",
    "import seaborn as sns\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "# Try to import plotly with fallback\n",
    "try:\n",
    "    import plotly.graph_objects as go\n",
    "    import plotly.express as px\n",
    "    from plotly.subplots import make_subplots\n",
    "    PLOTLY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"âš ï¸  Plotly not available, using matplotlib only\")\n",
    "    PLOTLY_AVAILABLE = False\n",
    "\n",
    "# Try to import interactive widgets with fallback\n",
    "try:\n",
    "    from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "    import ipywidgets as widgets\n",
    "    WIDGETS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"âš ï¸  IPywidgets not available, using static plots\")\n",
    "    WIDGETS_AVAILABLE = False\n",
    "    # Define dummy interact function\n",
    "    def interact(func, **kwargs):\n",
    "        # Call function with first value of each parameter\n",
    "        args = {}\n",
    "        for key, value in kwargs.items():\n",
    "            if isinstance(value, list):\n",
    "                args[key] = value[0]\n",
    "            elif hasattr(value, 'start'):  # range-like object\n",
    "                args[key] = value.start\n",
    "            else:\n",
    "                args[key] = value\n",
    "        return func(**args)\n",
    "\n",
    "# Utilities\n",
    "import time\n",
    "import os\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set style and random seeds for reproducibility\n",
    "try:\n",
    "    plt.style.use('seaborn-v0_8')\n",
    "except:\n",
    "    plt.style.use('seaborn')  # Fallback for older matplotlib\n",
    "    \n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"âœ“ Using device: {device}\")\n",
    "\n",
    "# Set up high-quality plots\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.grid'] = True\n",
    "\n",
    "print(\"âœ“ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5b0c689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Torchvision compatibility error: name 'transforms' is not defined\n",
      "\n",
      "ðŸ”§ Suggested fixes:\n",
      "1. Restart kernel and run: !pip install torch torchvision --upgrade\n",
      "2. Or install specific compatible versions:\n",
      "   !pip install torch==2.0.1 torchvision==0.15.2\n",
      "3. If in Colab: !pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu\n",
      "\n",
      "âš ï¸  Some features may be limited. The tutorial will continue with available functionality.\n"
     ]
    }
   ],
   "source": [
    "# Fix PyTorch/Torchvision compatibility if needed\n",
    "def check_and_fix_torch_compatibility():\n",
    "    \"\"\"\n",
    "    Check PyTorch and torchvision compatibility and provide fixes\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Test basic torchvision operations\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,), (0.5,))\n",
    "        ])\n",
    "        \n",
    "        # Test dataset loading capability\n",
    "        try:\n",
    "            # This will fail if torchvision has issues\n",
    "            test_dataset = torchvision.datasets.FakeData(\n",
    "                size=10, \n",
    "                image_size=(3, 32, 32),\n",
    "                transform=transform\n",
    "            )\n",
    "            test_loader = DataLoader(test_dataset, batch_size=2)\n",
    "            print(\"âœ“ Torchvision working correctly\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸  Torchvision dataset issue: {e}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Torchvision compatibility error: {e}\")\n",
    "        print(\"\\nðŸ”§ Suggested fixes:\")\n",
    "        print(\"1. Restart kernel and run: !pip install torch torchvision --upgrade\")\n",
    "        print(\"2. Or install specific compatible versions:\")\n",
    "        print(\"   !pip install torch==2.0.1 torchvision==0.15.2\")\n",
    "        print(\"3. If in Colab: !pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu\")\n",
    "        return False\n",
    "\n",
    "# Run compatibility check\n",
    "compatibility_ok = check_and_fix_torch_compatibility()\n",
    "\n",
    "if not compatibility_ok:\n",
    "    print(\"\\nâš ï¸  Some features may be limited. The tutorial will continue with available functionality.\")\n",
    "else:\n",
    "    print(\"ðŸŽ‰ Ready to start the CNN tutorial!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe7343b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Data loading functions ready\n"
     ]
    }
   ],
   "source": [
    "# Alternative data loading function with fallback\n",
    "def create_synthetic_cifar_data(num_samples=1000, batch_size=32):\n",
    "    \"\"\"\n",
    "    Create synthetic CIFAR-10 like data if torchvision fails\n",
    "    \"\"\"\n",
    "    print(\"Creating synthetic CIFAR-10 data for demonstration...\")\n",
    "    \n",
    "    # Create synthetic images (32x32x3)\n",
    "    images = torch.randn(num_samples, 3, 32, 32)\n",
    "    # Normalize to reasonable range\n",
    "    images = (images + 1) / 2  # Scale to [0, 1]\n",
    "    \n",
    "    # Create random labels (10 classes)\n",
    "    labels = torch.randint(0, 10, (num_samples,))\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    dataset = TensorDataset(images, labels)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # CIFAR-10 class names\n",
    "    classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "    \n",
    "    print(f\"âœ“ Created synthetic dataset with {num_samples} samples\")\n",
    "    return dataloader, dataloader, classes  # Return same for train/test\n",
    "\n",
    "def safe_load_cifar10_data(batch_size=32, num_workers=0):\n",
    "    \"\"\"\n",
    "    Safely load CIFAR-10 data with fallback to synthetic data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Data transformations\n",
    "        transform_train = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "        ])\n",
    "        \n",
    "        transform_test = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "        ])\n",
    "        \n",
    "        # Try to load real CIFAR-10\n",
    "        trainset = torchvision.datasets.CIFAR10(\n",
    "            root='./data', train=True, download=True, transform=transform_train\n",
    "        )\n",
    "        trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, \n",
    "                               num_workers=num_workers, pin_memory=False)\n",
    "        \n",
    "        testset = torchvision.datasets.CIFAR10(\n",
    "            root='./data', train=False, download=True, transform=transform_test\n",
    "        )\n",
    "        testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, \n",
    "                              num_workers=num_workers, pin_memory=False)\n",
    "        \n",
    "        classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "        \n",
    "        print(\"âœ“ Successfully loaded real CIFAR-10 dataset\")\n",
    "        return trainloader, testloader, classes\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  Could not load CIFAR-10: {e}\")\n",
    "        print(\"ðŸ“ Using synthetic data for demonstration...\")\n",
    "        return create_synthetic_cifar_data(num_samples=2000, batch_size=batch_size)\n",
    "\n",
    "print(\"âœ“ Data loading functions ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b76abac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Running compatibility tests...\n",
      "âœ“ PyTorch operations working: torch.Size([2, 3, 32, 32]) â†’ torch.Size([2, 16, 32, 32])\n",
      "âŒ Training test failed: No module named 'torch._dynamo'\n",
      "âœ“ Matplotlib working\n",
      "\n",
      "ðŸŽ¯ All systems ready! You can now proceed with the CNN tutorial.\n",
      "If you see any errors above, please install the suggested packages.\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Quick test to verify everything is working\n",
    "print(\"ðŸ” Running compatibility tests...\")\n",
    "\n",
    "# Test 1: PyTorch basic operations\n",
    "try:\n",
    "    test_tensor = torch.randn(2, 3, 32, 32)\n",
    "    test_conv = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "    test_output = test_conv(test_tensor)\n",
    "    print(f\"âœ“ PyTorch operations working: {test_tensor.shape} â†’ {test_output.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ PyTorch test failed: {e}\")\n",
    "\n",
    "# Test 2: Basic training setup\n",
    "try:\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(test_conv.parameters(), lr=0.001)\n",
    "    target = torch.randint(0, 10, (2,))\n",
    "    \n",
    "    # Dummy forward pass\n",
    "    dummy_logits = torch.randn(2, 10)\n",
    "    loss = criterion(dummy_logits, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"âœ“ Training components working: loss = {loss.item():.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Training test failed: {e}\")\n",
    "\n",
    "# Test 3: Visualization\n",
    "try:\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(4, 3))\n",
    "    ax.plot([1, 2, 3], [1, 4, 2])\n",
    "    ax.set_title(\"Test Plot\")\n",
    "    plt.close(fig)  # Close to save memory\n",
    "    print(\"âœ“ Matplotlib working\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Visualization test failed: {e}\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ All systems ready! You can now proceed with the CNN tutorial.\")\n",
    "print(\"If you see any errors above, please install the suggested packages.\")\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966cdf02",
   "metadata": {},
   "source": [
    "## 2. Understanding Convolution Operation\n",
    "\n",
    "The convolution operation is the heart of CNNs. Let's understand it mathematically and visually.\n",
    "\n",
    "### Mathematical Foundation\n",
    "\n",
    "For a 2D convolution, given an input $I$ and kernel $K$, the output $O$ at position $(i,j)$ is:\n",
    "\n",
    "$$O(i,j) = \\sum_{m=0}^{M-1} \\sum_{n=0}^{N-1} I(i+m, j+n) \\cdot K(m,n)$$\n",
    "\n",
    "Where:\n",
    "- $I$ is the input image/feature map\n",
    "- $K$ is the kernel/filter of size $M \\times N$\n",
    "- $O$ is the output feature map\n",
    "\n",
    "Let's implement this from scratch and visualize how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29299efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input matrix:\n",
      "[[ 1  2  3  4]\n",
      " [ 5  6  7  8]\n",
      " [ 9 10 11 12]\n",
      " [13 14 15 16]]\n",
      "\n",
      "Kernel (Sobel X):\n",
      "[[-1  0  1]\n",
      " [-2  0  2]\n",
      " [-1  0  1]]\n",
      "\n",
      "Convolution result:\n",
      "[[8. 8.]\n",
      " [8. 8.]]\n"
     ]
    }
   ],
   "source": [
    "def convolution_2d(input_matrix, kernel, stride=1, padding=0):\n",
    "    \"\"\"\n",
    "    Implement 2D convolution from scratch\n",
    "    \n",
    "    Args:\n",
    "        input_matrix: Input image/feature map (H x W)\n",
    "        kernel: Convolution kernel (K x K)\n",
    "        stride: Stride for convolution\n",
    "        padding: Zero padding\n",
    "    \n",
    "    Returns:\n",
    "        Output feature map\n",
    "    \"\"\"\n",
    "    # Add padding\n",
    "    if padding > 0:\n",
    "        input_matrix = np.pad(input_matrix, padding, mode='constant', constant_values=0)\n",
    "    \n",
    "    input_h, input_w = input_matrix.shape\n",
    "    kernel_h, kernel_w = kernel.shape\n",
    "    \n",
    "    # Calculate output dimensions\n",
    "    output_h = (input_h - kernel_h) // stride + 1\n",
    "    output_w = (input_w - kernel_w) // stride + 1\n",
    "    \n",
    "    # Initialize output\n",
    "    output = np.zeros((output_h, output_w))\n",
    "    \n",
    "    # Perform convolution\n",
    "    for i in range(0, output_h * stride, stride):\n",
    "        for j in range(0, output_w * stride, stride):\n",
    "            # Extract region of interest\n",
    "            roi = input_matrix[i:i+kernel_h, j:j+kernel_w]\n",
    "            # Element-wise multiplication and sum\n",
    "            output[i//stride, j//stride] = np.sum(roi * kernel)\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Test with a simple example\n",
    "test_input = np.array([\n",
    "    [1, 2, 3, 4],\n",
    "    [5, 6, 7, 8],\n",
    "    [9, 10, 11, 12],\n",
    "    [13, 14, 15, 16]\n",
    "])\n",
    "\n",
    "# Edge detection kernel (Sobel filter)\n",
    "sobel_x = np.array([\n",
    "    [-1, 0, 1],\n",
    "    [-2, 0, 2],\n",
    "    [-1, 0, 1]\n",
    "])\n",
    "\n",
    "# Apply convolution\n",
    "result = convolution_2d(test_input, sobel_x)\n",
    "print(\"Input matrix:\")\n",
    "print(test_input)\n",
    "print(\"\\nKernel (Sobel X):\")\n",
    "print(sobel_x)\n",
    "print(\"\\nConvolution result:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60b60319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 9 convolution steps\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x400 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def visualize_convolution_step_by_step(input_img, kernel, stride=1, padding=0):\n",
    "    \"\"\"\n",
    "    Create an animated visualization of the convolution process\n",
    "    \"\"\"\n",
    "    # Add padding if needed\n",
    "    if padding > 0:\n",
    "        padded_input = np.pad(input_img, padding, mode='constant', constant_values=0)\n",
    "    else:\n",
    "        padded_input = input_img.copy()\n",
    "    \n",
    "    input_h, input_w = padded_input.shape\n",
    "    kernel_h, kernel_w = kernel.shape\n",
    "    \n",
    "    output_h = (input_h - kernel_h) // stride + 1\n",
    "    output_w = (input_w - kernel_w) // stride + 1\n",
    "    output = np.zeros((output_h, output_w))\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "    \n",
    "    steps = []\n",
    "    for i in range(0, output_h * stride, stride):\n",
    "        for j in range(0, output_w * stride, stride):\n",
    "            roi = padded_input[i:i+kernel_h, j:j+kernel_w]\n",
    "            conv_result = np.sum(roi * kernel)\n",
    "            output[i//stride, j//stride] = conv_result\n",
    "            \n",
    "            steps.append({\n",
    "                'position': (i, j),\n",
    "                'roi': roi.copy(),\n",
    "                'result': conv_result,\n",
    "                'output': output.copy()\n",
    "            })\n",
    "    \n",
    "    def animate_step(step_idx):\n",
    "        for ax in axes:\n",
    "            ax.clear()\n",
    "        \n",
    "        step = steps[step_idx]\n",
    "        pos_i, pos_j = step['position']\n",
    "        \n",
    "        # Show input with highlighted region\n",
    "        axes[0].imshow(padded_input, cmap='Blues', alpha=0.7)\n",
    "        rect = Rectangle((pos_j-0.5, pos_i-0.5), kernel_w, kernel_h, \n",
    "                        linewidth=3, edgecolor='red', facecolor='none')\n",
    "        axes[0].add_patch(rect)\n",
    "        axes[0].set_title(f'Input (Step {step_idx+1})')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Show kernel\n",
    "        im1 = axes[1].imshow(kernel, cmap='RdBu', vmin=-2, vmax=2)\n",
    "        axes[1].set_title('Kernel')\n",
    "        for i in range(kernel_h):\n",
    "            for j in range(kernel_w):\n",
    "                axes[1].text(j, i, f'{kernel[i,j]:.1f}', \n",
    "                           ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "        \n",
    "        # Show region of interest\n",
    "        im2 = axes[2].imshow(step['roi'], cmap='Blues')\n",
    "        axes[2].set_title(f'ROI Ã— Kernel = {step[\"result\"]:.2f}')\n",
    "        for i in range(kernel_h):\n",
    "            for j in range(kernel_w):\n",
    "                product = step['roi'][i,j] * kernel[i,j]\n",
    "                axes[2].text(j, i, f'{product:.1f}', \n",
    "                           ha='center', va='center', fontsize=9, \n",
    "                           color='white' if step['roi'][i,j] > step['roi'].max()/2 else 'black')\n",
    "        \n",
    "        # Show current output\n",
    "        im3 = axes[3].imshow(step['output'], cmap='Greens')\n",
    "        axes[3].set_title('Output Feature Map')\n",
    "        output_i, output_j = pos_i//stride, pos_j//stride\n",
    "        axes[3].add_patch(Rectangle((output_j-0.5, output_i-0.5), 1, 1, \n",
    "                                  linewidth=3, edgecolor='red', facecolor='none'))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "    \n",
    "    return steps, animate_step\n",
    "\n",
    "# Create a simple test image\n",
    "test_image = np.array([\n",
    "    [0, 0, 1, 1, 0],\n",
    "    [0, 0, 1, 1, 0],\n",
    "    [0, 0, 1, 1, 0],\n",
    "    [1, 1, 1, 1, 1],\n",
    "    [1, 1, 0, 0, 0]\n",
    "])\n",
    "\n",
    "# Vertical edge detection kernel\n",
    "vertical_edge = np.array([\n",
    "    [-1, 0, 1],\n",
    "    [-1, 0, 1],\n",
    "    [-1, 0, 1]\n",
    "])\n",
    "\n",
    "steps, animate_func = visualize_convolution_step_by_step(test_image, vertical_edge)\n",
    "print(f\"Created {len(steps)} convolution steps\")\n",
    "\n",
    "# Show the first step\n",
    "animate_func(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3ec26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive convolution explorer\n",
    "def interactive_convolution(kernel_type='vertical_edge', stride=1, padding=0):\n",
    "    \"\"\"\n",
    "    Interactive widget to explore different kernels and parameters\n",
    "    \"\"\"\n",
    "    kernels = {\n",
    "        'vertical_edge': np.array([[-1, 0, 1], [-1, 0, 1], [-1, 0, 1]]),\n",
    "        'horizontal_edge': np.array([[-1, -1, -1], [0, 0, 0], [1, 1, 1]]),\n",
    "        'blur': np.ones((3,3)) / 9,\n",
    "        'sharpen': np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]]),\n",
    "        'identity': np.array([[0, 0, 0], [0, 1, 0], [0, 0, 0]])\n",
    "    }\n",
    "    \n",
    "    kernel = kernels[kernel_type]\n",
    "    \n",
    "    # Apply convolution\n",
    "    result = convolution_2d(test_image, kernel, stride=stride, padding=padding)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    # Original image\n",
    "    im1 = axes[0].imshow(test_image, cmap='Blues')\n",
    "    axes[0].set_title('Input Image')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    plt.colorbar(im1, ax=axes[0])\n",
    "    \n",
    "    # Kernel\n",
    "    im2 = axes[1].imshow(kernel, cmap='RdBu', vmin=-2, vmax=2)\n",
    "    axes[1].set_title(f'Kernel: {kernel_type}')\n",
    "    for i in range(kernel.shape[0]):\n",
    "        for j in range(kernel.shape[1]):\n",
    "            axes[1].text(j, i, f'{kernel[i,j]:.2f}', \n",
    "                        ha='center', va='center', fontweight='bold')\n",
    "    plt.colorbar(im2, ax=axes[1])\n",
    "    \n",
    "    # Result\n",
    "    im3 = axes[2].imshow(result, cmap='Greens')\n",
    "    axes[2].set_title(f'Output (stride={stride}, padding={padding})')\n",
    "    for i in range(result.shape[0]):\n",
    "        for j in range(result.shape[1]):\n",
    "            axes[2].text(j, i, f'{result[i,j]:.1f}', \n",
    "                        ha='center', va='center', fontweight='bold',\n",
    "                        color='white' if abs(result[i,j]) > abs(result).max()/2 else 'black')\n",
    "    plt.colorbar(im3, ax=axes[2])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    print(f\"Input shape: {test_image.shape}\")\n",
    "    print(f\"Kernel shape: {kernel.shape}\")\n",
    "    print(f\"Output shape: {result.shape}\")\n",
    "\n",
    "# Create interactive widget\n",
    "interact(interactive_convolution, \n",
    "         kernel_type=['vertical_edge', 'horizontal_edge', 'blur', 'sharpen', 'identity'],\n",
    "         stride=(1, 3, 1),\n",
    "         padding=(0, 2, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9879b7c8",
   "metadata": {},
   "source": [
    "## 3. Building Convolutional Layers\n",
    "\n",
    "Now let's understand how convolutional layers work in deep learning frameworks and how different parameters affect the output.\n",
    "\n",
    "### Key Parameters:\n",
    "- **Stride**: How much the kernel moves between applications\n",
    "- **Padding**: Zero-padding added to input borders  \n",
    "- **Dilation**: Spacing between kernel elements (atrous convolution)\n",
    "- **Groups**: Number of blocked connections from input to output channels\n",
    "\n",
    "### Mathematical Relationships:\n",
    "\n",
    "For input size $H_{in} \\times W_{in}$, kernel size $K$, padding $P$, stride $S$, and dilation $D$:\n",
    "\n",
    "$$H_{out} = \\lfloor \\frac{H_{in} + 2P - D(K-1) - 1}{S} \\rfloor + 1$$\n",
    "\n",
    "$$W_{out} = \\lfloor \\frac{W_{in} + 2P - D(K-1) - 1}{S} \\rfloor + 1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da0b61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalLayer:\n",
    "    \"\"\"\n",
    "    A complete convolutional layer implementation with all parameters\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, \n",
    "                 padding=0, dilation=1, bias=True):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size if isinstance(kernel_size, tuple) else (kernel_size, kernel_size)\n",
    "        self.stride = stride if isinstance(stride, tuple) else (stride, stride)\n",
    "        self.padding = padding if isinstance(padding, tuple) else (padding, padding)\n",
    "        self.dilation = dilation if isinstance(dilation, tuple) else (dilation, dilation)\n",
    "        \n",
    "        # Initialize weights and bias\n",
    "        self.weights = np.random.randn(out_channels, in_channels, *self.kernel_size) * 0.1\n",
    "        self.bias = np.random.randn(out_channels) * 0.1 if bias else None\n",
    "    \n",
    "    def calculate_output_shape(self, input_shape):\n",
    "        \"\"\"Calculate output shape given input shape\"\"\"\n",
    "        batch_size, channels, height, width = input_shape\n",
    "        \n",
    "        # Effective kernel size with dilation\n",
    "        eff_kernel_h = (self.kernel_size[0] - 1) * self.dilation[0] + 1\n",
    "        eff_kernel_w = (self.kernel_size[1] - 1) * self.dilation[1] + 1\n",
    "        \n",
    "        # Output dimensions\n",
    "        out_height = (height + 2 * self.padding[0] - eff_kernel_h) // self.stride[0] + 1\n",
    "        out_width = (width + 2 * self.padding[1] - eff_kernel_w) // self.stride[1] + 1\n",
    "        \n",
    "        return (batch_size, self.out_channels, out_height, out_width)\n",
    "    \n",
    "    def visualize_layer_effect(self, input_shape=(1, 3, 32, 32)):\n",
    "        \"\"\"Visualize how layer parameters affect output dimensions\"\"\"\n",
    "        output_shape = self.calculate_output_shape(input_shape)\n",
    "        \n",
    "        fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "        \n",
    "        # Input representation\n",
    "        input_rect = Rectangle((1, 4), input_shape[2]/8, input_shape[3]/8, \n",
    "                              linewidth=2, edgecolor='blue', facecolor='lightblue', alpha=0.7)\n",
    "        ax.add_patch(input_rect)\n",
    "        ax.text(1 + input_shape[2]/16, 4 + input_shape[3]/16, \n",
    "                f'Input\\n{input_shape[2]}Ã—{input_shape[3]}Ã—{input_shape[1]}', \n",
    "                ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "        \n",
    "        # Kernel representation\n",
    "        kernel_rect = Rectangle((3, 3), self.kernel_size[0]/4, self.kernel_size[1]/4,\n",
    "                               linewidth=2, edgecolor='red', facecolor='lightcoral', alpha=0.7)\n",
    "        ax.add_patch(kernel_rect)\n",
    "        ax.text(3 + self.kernel_size[0]/8, 3 + self.kernel_size[1]/8,\n",
    "                f'Kernel\\n{self.kernel_size[0]}Ã—{self.kernel_size[1]}', \n",
    "                ha='center', va='center', fontsize=9, fontweight='bold')\n",
    "        \n",
    "        # Output representation\n",
    "        output_rect = Rectangle((6, 4), output_shape[2]/8, output_shape[3]/8,\n",
    "                               linewidth=2, edgecolor='green', facecolor='lightgreen', alpha=0.7)\n",
    "        ax.add_patch(output_rect)\n",
    "        ax.text(6 + output_shape[2]/16, 4 + output_shape[3]/16,\n",
    "                f'Output\\n{output_shape[2]}Ã—{output_shape[3]}Ã—{output_shape[1]}', \n",
    "                ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "        \n",
    "        # Parameters text\n",
    "        param_text = f\"\"\"Parameters:\n",
    "        Kernel: {self.kernel_size[0]}Ã—{self.kernel_size[1]}\n",
    "        Stride: {self.stride[0]}Ã—{self.stride[1]}\n",
    "        Padding: {self.padding[0]}Ã—{self.padding[1]}\n",
    "        Dilation: {self.dilation[0]}Ã—{self.dilation[1]}\n",
    "        \n",
    "        Channels: {self.in_channels} â†’ {self.out_channels}\n",
    "        \"\"\"\n",
    "        \n",
    "        ax.text(1, 1, param_text, fontsize=10, \n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightyellow\"))\n",
    "        \n",
    "        # Arrows\n",
    "        ax.arrow(2.5, 4.5, 0.4, 0, head_width=0.1, head_length=0.1, fc='black', ec='black')\n",
    "        ax.arrow(4.8, 4.5, 0.4, 0, head_width=0.1, head_length=0.1, fc='black', ec='black')\n",
    "        \n",
    "        ax.set_xlim(0, 9)\n",
    "        ax.set_ylim(0, 7)\n",
    "        ax.set_aspect('equal')\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f'Convolutional Layer: {input_shape} â†’ {output_shape}', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return output_shape\n",
    "\n",
    "# Example usage\n",
    "layer1 = ConvolutionalLayer(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "output_shape1 = layer1.visualize_layer_effect((1, 3, 32, 32))\n",
    "\n",
    "layer2 = ConvolutionalLayer(in_channels=32, out_channels=64, kernel_size=3, stride=2, padding=1)\n",
    "output_shape2 = layer2.visualize_layer_effect(output_shape1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1063c556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive parameter explorer for convolutional layers\n",
    "def explore_conv_parameters(kernel_size=3, stride=1, padding=0, dilation=1, \n",
    "                          input_height=32, input_width=32, in_channels=3, out_channels=32):\n",
    "    \"\"\"\n",
    "    Interactive exploration of convolutional layer parameters\n",
    "    \"\"\"\n",
    "    # Calculate output dimensions\n",
    "    eff_kernel_h = (kernel_size - 1) * dilation + 1\n",
    "    eff_kernel_w = (kernel_size - 1) * dilation + 1\n",
    "    \n",
    "    out_height = (input_height + 2 * padding - eff_kernel_h) // stride + 1\n",
    "    out_width = (input_width + 2 * padding - eff_kernel_w) // stride + 1\n",
    "    \n",
    "    # Calculate number of parameters\n",
    "    num_params = out_channels * in_channels * kernel_size * kernel_size + out_channels\n",
    "    \n",
    "    # Calculate receptive field\n",
    "    receptive_field = (kernel_size - 1) * dilation + 1\n",
    "    \n",
    "    print(f\"Input: {input_height} Ã— {input_width} Ã— {in_channels}\")\n",
    "    print(f\"Output: {out_height} Ã— {out_width} Ã— {out_channels}\")\n",
    "    print(f\"Parameters: {num_params:,}\")\n",
    "    print(f\"Effective kernel size: {eff_kernel_h} Ã— {eff_kernel_w}\")\n",
    "    print(f\"Receptive field: {receptive_field} Ã— {receptive_field}\")\n",
    "    \n",
    "    # Visualize the effect\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Input visualization\n",
    "    axes[0].add_patch(Rectangle((0, 0), input_width, input_height, \n",
    "                               linewidth=2, edgecolor='blue', facecolor='lightblue', alpha=0.7))\n",
    "    axes[0].set_xlim(-2, input_width + 2)\n",
    "    axes[0].set_ylim(-2, input_height + 2)\n",
    "    axes[0].set_title(f'Input: {input_height}Ã—{input_width}Ã—{in_channels}')\n",
    "    axes[0].set_aspect('equal')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Kernel with dilation visualization\n",
    "    for i in range(kernel_size):\n",
    "        for j in range(kernel_size):\n",
    "            x = j * dilation\n",
    "            y = i * dilation\n",
    "            axes[1].add_patch(Rectangle((x, y), 1, 1, \n",
    "                                       linewidth=1, edgecolor='red', facecolor='lightcoral'))\n",
    "    \n",
    "    axes[1].set_xlim(-1, max(5, (kernel_size-1)*dilation + 2))\n",
    "    axes[1].set_ylim(-1, max(5, (kernel_size-1)*dilation + 2))\n",
    "    axes[1].set_title(f'Kernel: {kernel_size}Ã—{kernel_size}, Dilation: {dilation}')\n",
    "    axes[1].set_aspect('equal')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Output visualization\n",
    "    axes[2].add_patch(Rectangle((0, 0), out_width, out_height, \n",
    "                               linewidth=2, edgecolor='green', facecolor='lightgreen', alpha=0.7))\n",
    "    axes[2].set_xlim(-2, max(out_width + 2, 5))\n",
    "    axes[2].set_ylim(-2, max(out_height + 2, 5))\n",
    "    axes[2].set_title(f'Output: {out_height}Ã—{out_width}Ã—{out_channels}')\n",
    "    axes[2].set_aspect('equal')\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "# Create interactive widget\n",
    "interact(explore_conv_parameters,\n",
    "         kernel_size=(1, 7, 2),\n",
    "         stride=(1, 4, 1),\n",
    "         padding=(0, 3, 1),\n",
    "         dilation=(1, 3, 1),\n",
    "         input_height=fixed(32),\n",
    "         input_width=fixed(32),\n",
    "         in_channels=(1, 64, 1),\n",
    "         out_channels=(1, 128, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c182e75",
   "metadata": {},
   "source": [
    "## 4. Implementing Pooling Operations\n",
    "\n",
    "Pooling layers reduce spatial dimensions while retaining important information. They help with:\n",
    "- **Translation invariance**: Small shifts in input don't drastically change output\n",
    "- **Computational efficiency**: Smaller feature maps\n",
    "- **Overfitting reduction**: Act as regularization\n",
    "\n",
    "### Types of Pooling:\n",
    "1. **Max Pooling**: Takes maximum value in each window\n",
    "2. **Average Pooling**: Takes mean value in each window  \n",
    "3. **Adaptive Pooling**: Output size is fixed regardless of input size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec84b9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pooling_2d(input_matrix, pool_size, stride=None):\n",
    "    \"\"\"\n",
    "    Implement 2D max pooling\n",
    "    \"\"\"\n",
    "    if stride is None:\n",
    "        stride = pool_size\n",
    "    \n",
    "    input_h, input_w = input_matrix.shape\n",
    "    pool_h, pool_w = pool_size\n",
    "    \n",
    "    output_h = (input_h - pool_h) // stride + 1\n",
    "    output_w = (input_w - pool_w) // stride + 1\n",
    "    \n",
    "    output = np.zeros((output_h, output_w))\n",
    "    \n",
    "    for i in range(0, output_h * stride, stride):\n",
    "        for j in range(0, output_w * stride, stride):\n",
    "            pool_region = input_matrix[i:i+pool_h, j:j+pool_w]\n",
    "            output[i//stride, j//stride] = np.max(pool_region)\n",
    "    \n",
    "    return output\n",
    "\n",
    "def average_pooling_2d(input_matrix, pool_size, stride=None):\n",
    "    \"\"\"\n",
    "    Implement 2D average pooling\n",
    "    \"\"\"\n",
    "    if stride is None:\n",
    "        stride = pool_size\n",
    "    \n",
    "    input_h, input_w = input_matrix.shape\n",
    "    pool_h, pool_w = pool_size\n",
    "    \n",
    "    output_h = (input_h - pool_h) // stride + 1\n",
    "    output_w = (input_w - pool_w) // stride + 1\n",
    "    \n",
    "    output = np.zeros((output_h, output_w))\n",
    "    \n",
    "    for i in range(0, output_h * stride, stride):\n",
    "        for j in range(0, output_w * stride, stride):\n",
    "            pool_region = input_matrix[i:i+pool_h, j:j+pool_w]\n",
    "            output[i//stride, j//stride] = np.mean(pool_region)\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Test pooling operations\n",
    "test_feature_map = np.random.randn(8, 8)\n",
    "\n",
    "max_pooled = max_pooling_2d(test_feature_map, (2, 2))\n",
    "avg_pooled = average_pooling_2d(test_feature_map, (2, 2))\n",
    "\n",
    "# Visualize pooling effects\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "im1 = axes[0].imshow(test_feature_map, cmap='viridis')\n",
    "axes[0].set_title('Original Feature Map (8Ã—8)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "plt.colorbar(im1, ax=axes[0])\n",
    "\n",
    "im2 = axes[1].imshow(max_pooled, cmap='viridis')\n",
    "axes[1].set_title('Max Pooled (4Ã—4)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "plt.colorbar(im2, ax=axes[1])\n",
    "\n",
    "im3 = axes[2].imshow(avg_pooled, cmap='viridis')\n",
    "axes[2].set_title('Average Pooled (4Ã—4)')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "plt.colorbar(im3, ax=axes[2])\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "print(f\"Original shape: {test_feature_map.shape}\")\n",
    "print(f\"Max pooled shape: {max_pooled.shape}\")\n",
    "print(f\"Average pooled shape: {avg_pooled.shape}\")\n",
    "print(f\"Information retention - Max: {(max_pooled.size / test_feature_map.size) * 100:.1f}%\")\n",
    "print(f\"Information retention - Avg: {(avg_pooled.size / test_feature_map.size) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cc66b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_pooling_operation(input_img, pool_size=(2, 2), operation='max'):\n",
    "    \"\"\"\n",
    "    Visualize pooling operation step by step\n",
    "    \"\"\"\n",
    "    input_h, input_w = input_img.shape\n",
    "    pool_h, pool_w = pool_size\n",
    "    \n",
    "    output_h = input_h // pool_h\n",
    "    output_w = input_w // pool_w\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    \n",
    "    # Original image\n",
    "    im1 = axes[0, 0].imshow(input_img, cmap='Blues')\n",
    "    axes[0, 0].set_title('Input Feature Map')\n",
    "    plt.colorbar(im1, ax=axes[0, 0])\n",
    "    \n",
    "    # Show pooling regions\n",
    "    axes[0, 1].imshow(input_img, cmap='Blues', alpha=0.7)\n",
    "    for i in range(0, input_h, pool_h):\n",
    "        for j in range(0, input_w, pool_w):\n",
    "            rect = Rectangle((j-0.5, i-0.5), pool_w, pool_h, \n",
    "                           linewidth=2, edgecolor='red', facecolor='none')\n",
    "            axes[0, 1].add_patch(rect)\n",
    "    axes[0, 1].set_title(f'Pooling Regions ({pool_h}Ã—{pool_w})')\n",
    "    \n",
    "    # Perform pooling\n",
    "    if operation == 'max':\n",
    "        pooled = max_pooling_2d(input_img, pool_size)\n",
    "        op_name = 'Max'\n",
    "    else:\n",
    "        pooled = average_pooling_2d(input_img, pool_size)\n",
    "        op_name = 'Average'\n",
    "    \n",
    "    # Show result\n",
    "    im3 = axes[0, 2].imshow(pooled, cmap='Blues')\n",
    "    axes[0, 2].set_title(f'{op_name} Pooled Output')\n",
    "    plt.colorbar(im3, ax=axes[0, 2])\n",
    "    \n",
    "    # Detailed view of a few pooling operations\n",
    "    examples = [(0, 0), (0, 2), (2, 0)]\n",
    "    for idx, (i, j) in enumerate(examples):\n",
    "        if i < input_h - pool_h and j < input_w - pool_w:\n",
    "            region = input_img[i:i+pool_h, j:j+pool_w]\n",
    "            \n",
    "            axes[1, idx].imshow(region, cmap='Blues')\n",
    "            axes[1, idx].set_title(f'Region ({i},{j}) â†’ {pooled[i//pool_h, j//pool_w]:.2f}')\n",
    "            \n",
    "            for x in range(pool_h):\n",
    "                for y in range(pool_w):\n",
    "                    value = region[x, y]\n",
    "                    color = 'white' if value < region.max()/2 else 'black'\n",
    "                    if operation == 'max' and value == region.max():\n",
    "                        color = 'red'\n",
    "                        weight = 'bold'\n",
    "                    else:\n",
    "                        weight = 'normal'\n",
    "                    axes[1, idx].text(y, x, f'{value:.2f}', \n",
    "                                    ha='center', va='center', \n",
    "                                    color=color, fontweight=weight)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return pooled\n",
    "\n",
    "# Create a sample feature map with clear patterns\n",
    "sample_feature = np.array([\n",
    "    [1.0, 2.0, 3.0, 1.0],\n",
    "    [4.0, 5.0, 2.0, 3.0],\n",
    "    [2.0, 1.0, 4.0, 5.0],\n",
    "    [3.0, 2.0, 1.0, 2.0]\n",
    "])\n",
    "\n",
    "print(\"Max Pooling:\")\n",
    "max_result = visualize_pooling_operation(sample_feature, (2, 2), 'max')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd499250",
   "metadata": {},
   "source": [
    "## 5. Visualizing Feature Maps\n",
    "\n",
    "Feature maps are the outputs of convolutional layers - they show what patterns the network has learned to detect. Let's create some synthetic data and see how different filters respond to different patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56371d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_synthetic_images():\n",
    "    \"\"\"Create synthetic images with different patterns\"\"\"\n",
    "    \n",
    "    # Vertical stripes\n",
    "    vertical = np.zeros((32, 32))\n",
    "    vertical[:, ::4] = 1\n",
    "    \n",
    "    # Horizontal stripes  \n",
    "    horizontal = np.zeros((32, 32))\n",
    "    horizontal[::4, :] = 1\n",
    "    \n",
    "    # Diagonal pattern\n",
    "    diagonal = np.zeros((32, 32))\n",
    "    for i in range(32):\n",
    "        for j in range(32):\n",
    "            if (i + j) % 8 < 2:\n",
    "                diagonal[i, j] = 1\n",
    "    \n",
    "    # Checkerboard\n",
    "    checkerboard = np.zeros((32, 32))\n",
    "    for i in range(32):\n",
    "        for j in range(32):\n",
    "            if (i//4 + j//4) % 2 == 0:\n",
    "                checkerboard[i, j] = 1\n",
    "    \n",
    "    # Circular pattern\n",
    "    circular = np.zeros((32, 32))\n",
    "    center = (16, 16)\n",
    "    for i in range(32):\n",
    "        for j in range(32):\n",
    "            dist = np.sqrt((i - center[0])**2 + (j - center[1])**2)\n",
    "            if 8 < dist < 12:\n",
    "                circular[i, j] = 1\n",
    "    \n",
    "    return {\n",
    "        'vertical': vertical,\n",
    "        'horizontal': horizontal, \n",
    "        'diagonal': diagonal,\n",
    "        'checkerboard': checkerboard,\n",
    "        'circular': circular\n",
    "    }\n",
    "\n",
    "def apply_multiple_filters(image, filters_dict):\n",
    "    \"\"\"Apply multiple filters to an image and visualize results\"\"\"\n",
    "    \n",
    "    n_filters = len(filters_dict)\n",
    "    fig, axes = plt.subplots(2, n_filters + 1, figsize=(4 * (n_filters + 1), 8))\n",
    "    \n",
    "    # Show original image\n",
    "    axes[0, 0].imshow(image, cmap='gray')\n",
    "    axes[0, 0].set_title('Original Image')\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    # Empty cell in bottom row\n",
    "    axes[1, 0].axis('off')\n",
    "    \n",
    "    # Apply each filter\n",
    "    for idx, (filter_name, filter_kernel) in enumerate(filters_dict.items()):\n",
    "        # Apply convolution\n",
    "        filtered = convolution_2d(image, filter_kernel, padding=1)\n",
    "        \n",
    "        # Show filter\n",
    "        im1 = axes[0, idx + 1].imshow(filter_kernel, cmap='RdBu', vmin=-2, vmax=2)\n",
    "        axes[0, idx + 1].set_title(f'{filter_name.title()} Filter')\n",
    "        axes[0, idx + 1].axis('off')\n",
    "        \n",
    "        # Add values to filter visualization\n",
    "        for i in range(filter_kernel.shape[0]):\n",
    "            for j in range(filter_kernel.shape[1]):\n",
    "                axes[0, idx + 1].text(j, i, f'{filter_kernel[i,j]:.1f}', \n",
    "                                    ha='center', va='center', fontweight='bold',\n",
    "                                    color='white' if abs(filter_kernel[i,j]) > 1 else 'black')\n",
    "        \n",
    "        # Show filtered result\n",
    "        im2 = axes[1, idx + 1].imshow(filtered, cmap='RdBu')\n",
    "        axes[1, idx + 1].set_title(f'Filtered Result')\n",
    "        axes[1, idx + 1].axis('off')\n",
    "        \n",
    "        # Print activation statistics\n",
    "        print(f\"{filter_name.title()} filter response:\")\n",
    "        print(f\"  Max activation: {filtered.max():.2f}\")\n",
    "        print(f\"  Min activation: {filtered.min():.2f}\")\n",
    "        print(f\"  Mean absolute activation: {np.abs(filtered).mean():.2f}\")\n",
    "        print()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "# Create synthetic images\n",
    "synthetic_images = create_synthetic_images()\n",
    "\n",
    "# Define different edge detection filters\n",
    "filters = {\n",
    "    'vertical_edge': np.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]]),\n",
    "    'horizontal_edge': np.array([[-1, -2, -1], [0, 0, 0], [1, 2, 1]]),\n",
    "    'diagonal_edge': np.array([[0, 1, 2], [-1, 0, 1], [-2, -1, 0]]),\n",
    "    'laplacian': np.array([[0, -1, 0], [-1, 4, -1], [0, -1, 0]])\n",
    "}\n",
    "\n",
    "# Test with vertical stripes\n",
    "print(\"Testing filters on vertical stripes:\")\n",
    "apply_multiple_filters(synthetic_images['vertical'], filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3056803b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive feature map explorer\n",
    "def interactive_feature_explorer(image_pattern='vertical', filter_type='vertical_edge'):\n",
    "    \"\"\"\n",
    "    Interactive widget to explore how different filters respond to different patterns\n",
    "    \"\"\"\n",
    "    \n",
    "    image = synthetic_images[image_pattern]\n",
    "    filter_kernel = filters[filter_type]\n",
    "    \n",
    "    # Apply convolution\n",
    "    filtered = convolution_2d(image, filter_kernel, padding=1)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    # Original image\n",
    "    axes[0].imshow(image, cmap='gray')\n",
    "    axes[0].set_title(f'Input: {image_pattern.title()} Pattern')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Filter\n",
    "    im1 = axes[1].imshow(filter_kernel, cmap='RdBu', vmin=-2, vmax=2)\n",
    "    axes[1].set_title(f'Filter: {filter_type.replace(\"_\", \" \").title()}')\n",
    "    for i in range(filter_kernel.shape[0]):\n",
    "        for j in range(filter_kernel.shape[1]):\n",
    "            axes[1].text(j, i, f'{filter_kernel[i,j]:.1f}', \n",
    "                        ha='center', va='center', fontweight='bold',\n",
    "                        color='white' if abs(filter_kernel[i,j]) > 1 else 'black')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Result\n",
    "    im2 = axes[2].imshow(filtered, cmap='RdBu')\n",
    "    axes[2].set_title('Feature Map Output')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Statistics\n",
    "    activation_strength = np.abs(filtered).mean()\n",
    "    max_response = filtered.max()\n",
    "    min_response = filtered.min()\n",
    "    \n",
    "    print(f\"Filter Response Analysis:\")\n",
    "    print(f\"Maximum activation: {max_response:.2f}\")\n",
    "    print(f\"Minimum activation: {min_response:.2f}\")\n",
    "    print(f\"Average absolute activation: {activation_strength:.2f}\")\n",
    "    print(f\"Response strength: {'Strong' if activation_strength > 2 else 'Medium' if activation_strength > 1 else 'Weak'}\")\n",
    "\n",
    "# Create interactive widget\n",
    "interact(interactive_feature_explorer,\n",
    "         image_pattern=list(synthetic_images.keys()),\n",
    "         filter_type=list(filters.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bbc25f",
   "metadata": {},
   "source": [
    "## 6. Constructing a Complete CNN Architecture\n",
    "\n",
    "Now let's build a complete CNN using PyTorch. We'll create a modular architecture similar to LeNet-5 but with modern improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c653fc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModernLeNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A modern version of LeNet with additional features:\n",
    "    - Batch normalization\n",
    "    - Dropout for regularization\n",
    "    - ReLU activations\n",
    "    - Configurable architecture\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=10, input_channels=1, dropout_rate=0.5):\n",
    "        super(ModernLeNet, self).__init__()\n",
    "        \n",
    "        # Feature extraction layers\n",
    "        self.features = nn.Sequential(\n",
    "            # First convolutional block\n",
    "            nn.Conv2d(input_channels, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            # Second convolutional block  \n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            # Third convolutional block\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        \n",
    "        # Classifier layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(128 * 4 * 4, 256),  # Assuming 32x32 input\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, 84),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(84, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize network weights using Xavier/He initialization\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the network\"\"\"\n",
    "        # Extract features\n",
    "        x = self.features(x)\n",
    "        \n",
    "        # Flatten for classifier\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Classify\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def get_feature_maps(self, x, layer_idx=None):\n",
    "        \"\"\"\n",
    "        Extract feature maps from intermediate layers\n",
    "        \"\"\"\n",
    "        feature_maps = []\n",
    "        \n",
    "        for i, layer in enumerate(self.features):\n",
    "            x = layer(x)\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                feature_maps.append(x.clone())\n",
    "                if layer_idx is not None and len(feature_maps) > layer_idx:\n",
    "                    break\n",
    "        \n",
    "        return feature_maps\n",
    "\n",
    "# Create model instance\n",
    "model = ModernLeNet(num_classes=10, input_channels=1)\n",
    "\n",
    "# Print model architecture\n",
    "print(\"Model Architecture:\")\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783a549d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_network_architecture(model, input_shape=(1, 1, 32, 32)):\n",
    "    \"\"\"\n",
    "    Visualize the network architecture with layer details\n",
    "    \"\"\"\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(16, 10))\n",
    "    \n",
    "    # Create dummy input to track shapes\n",
    "    x = torch.randn(input_shape)\n",
    "    \n",
    "    # Track layer information\n",
    "    layers_info = []\n",
    "    current_shape = input_shape\n",
    "    \n",
    "    # Process feature extraction layers\n",
    "    for i, layer in enumerate(model.features):\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            # Calculate output shape for conv layer\n",
    "            in_c, in_h, in_w = current_shape[1:]\n",
    "            out_c = layer.out_channels\n",
    "            kernel_size = layer.kernel_size[0]\n",
    "            stride = layer.stride[0]\n",
    "            padding = layer.padding[0]\n",
    "            \n",
    "            out_h = (in_h + 2*padding - kernel_size) // stride + 1\n",
    "            out_w = (in_w + 2*padding - kernel_size) // stride + 1\n",
    "            current_shape = (current_shape[0], out_c, out_h, out_w)\n",
    "            \n",
    "            layers_info.append({\n",
    "                'type': 'Conv2d',\n",
    "                'params': f'{layer.in_channels}â†’{layer.out_channels}, K:{kernel_size}Ã—{kernel_size}',\n",
    "                'shape': f'{out_h}Ã—{out_w}Ã—{out_c}',\n",
    "                'position': len(layers_info) * 2\n",
    "            })\n",
    "            \n",
    "        elif isinstance(layer, nn.MaxPool2d):\n",
    "            # Calculate output shape for pooling\n",
    "            in_c, in_h, in_w = current_shape[1:]\n",
    "            kernel_size = layer.kernel_size\n",
    "            stride = layer.stride\n",
    "            \n",
    "            out_h = in_h // stride\n",
    "            out_w = in_w // stride\n",
    "            current_shape = (current_shape[0], in_c, out_h, out_w)\n",
    "            \n",
    "            layers_info.append({\n",
    "                'type': 'MaxPool2d',\n",
    "                'params': f'K:{kernel_size}Ã—{kernel_size}, S:{stride}',\n",
    "                'shape': f'{out_h}Ã—{out_w}Ã—{in_c}',\n",
    "                'position': len(layers_info) * 2\n",
    "            })\n",
    "    \n",
    "    # Add classifier info\n",
    "    flattened_size = current_shape[1] * current_shape[2] * current_shape[3]\n",
    "    for layer in model.classifier:\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            layers_info.append({\n",
    "                'type': 'Linear',\n",
    "                'params': f'{layer.in_features}â†’{layer.out_features}',\n",
    "                'shape': f'{layer.out_features}',\n",
    "                'position': len(layers_info) * 2\n",
    "            })\n",
    "    \n",
    "    # Draw the architecture\n",
    "    y_positions = [4, 3, 2, 1, 0]  # Different heights for different layer types\n",
    "    colors = {'Conv2d': 'lightblue', 'MaxPool2d': 'lightcoral', 'Linear': 'lightgreen'}\n",
    "    \n",
    "    for i, layer_info in enumerate(layers_info):\n",
    "        x_pos = layer_info['position']\n",
    "        \n",
    "        if layer_info['type'] in ['Conv2d', 'MaxPool2d']:\n",
    "            y_pos = 3 if layer_info['type'] == 'Conv2d' else 2\n",
    "            width = 1.5\n",
    "            height = 0.8\n",
    "        else:  # Linear layers\n",
    "            y_pos = 1\n",
    "            width = 1.2\n",
    "            height = 0.6\n",
    "        \n",
    "        # Draw rectangle\n",
    "        rect = Rectangle((x_pos, y_pos - height/2), width, height,\n",
    "                        linewidth=2, edgecolor='black', \n",
    "                        facecolor=colors[layer_info['type']], alpha=0.7)\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        # Add text\n",
    "        ax.text(x_pos + width/2, y_pos, f\"{layer_info['type']}\\\\n{layer_info['params']}\\\\n{layer_info['shape']}\", \n",
    "                ha='center', va='center', fontsize=8, fontweight='bold')\n",
    "        \n",
    "        # Add arrows\n",
    "        if i < len(layers_info) - 1:\n",
    "            ax.arrow(x_pos + width + 0.1, y_pos, 0.3, 0, \n",
    "                    head_width=0.1, head_length=0.1, fc='black', ec='black')\n",
    "    \n",
    "    ax.set_xlim(-0.5, max([info['position'] for info in layers_info]) + 2)\n",
    "    ax.set_ylim(0, 4.5)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    ax.set_title('CNN Architecture Visualization', fontsize=16, fontweight='bold', pad=20)\n",
    "    \n",
    "    # Add legend\n",
    "    legend_elements = [Rectangle((0, 0), 1, 1, facecolor=color, edgecolor='black', alpha=0.7, label=layer_type) \n",
    "                      for layer_type, color in colors.items()]\n",
    "    ax.legend(handles=legend_elements, loc='upper right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "# Visualize the architecture\n",
    "visualize_network_architecture(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f899d1",
   "metadata": {},
   "source": [
    "## 7. Forward Pass Implementation\n",
    "\n",
    "Let's trace through a forward pass step by step, monitoring tensor shapes and visualizing the data flow through our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98862c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_forward_pass(model, input_tensor, verbose=True):\n",
    "    \"\"\"\n",
    "    Perform forward pass with detailed logging of tensor shapes and operations\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Input shape: {input_tensor.shape}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    x = input_tensor\n",
    "    layer_outputs = []\n",
    "    \n",
    "    # Process feature extraction layers\n",
    "    for i, layer in enumerate(model.features):\n",
    "        x = layer(x)\n",
    "        layer_outputs.append(x.clone().detach())\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Layer {i+1}: {layer.__class__.__name__}\")\n",
    "            if hasattr(layer, 'weight') and layer.weight is not None:\n",
    "                print(f\"  Parameters: {layer.weight.shape}\")\n",
    "            print(f\"  Output shape: {x.shape}\")\n",
    "            print(f\"  Memory usage: {x.numel() * 4 / (1024**2):.2f} MB\")\n",
    "            \n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                print(f\"  Kernel size: {layer.kernel_size}\")\n",
    "                print(f\"  Stride: {layer.stride}\")\n",
    "                print(f\"  Padding: {layer.padding}\")\n",
    "            elif isinstance(layer, nn.MaxPool2d):\n",
    "                print(f\"  Pool size: {layer.kernel_size}\")\n",
    "                print(f\"  Pool stride: {layer.stride}\")\n",
    "            \n",
    "            print(\"-\" * 30)\n",
    "    \n",
    "    # Flatten for classifier\n",
    "    features_shape = x.shape\n",
    "    x = x.view(x.size(0), -1)\n",
    "    if verbose:\n",
    "        print(f\"Flattening: {features_shape} â†’ {x.shape}\")\n",
    "        print(\"-\" * 30)\n",
    "    \n",
    "    # Process classifier layers\n",
    "    for i, layer in enumerate(model.classifier):\n",
    "        x = layer(x)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Classifier Layer {i+1}: {layer.__class__.__name__}\")\n",
    "            if hasattr(layer, 'weight') and layer.weight is not None:\n",
    "                print(f\"  Parameters: {layer.weight.shape}\")\n",
    "            print(f\"  Output shape: {x.shape}\")\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                print(f\"  Input features: {layer.in_features}\")\n",
    "                print(f\"  Output features: {layer.out_features}\")\n",
    "            print(\"-\" * 30)\n",
    "    \n",
    "    print(f\"Final output shape: {x.shape}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    return x, layer_outputs\n",
    "\n",
    "# Create a random input tensor (batch_size=2 for demonstration)\n",
    "sample_input = torch.randn(2, 1, 32, 32)\n",
    "\n",
    "# Perform detailed forward pass\n",
    "model.eval()  # Set to evaluation mode\n",
    "with torch.no_grad():\n",
    "    output, feature_maps = detailed_forward_pass(model, sample_input)\n",
    "\n",
    "print(f\"\\\\nOutput probabilities shape: {output.shape}\")\n",
    "print(f\"Sample output (before softmax): {output[0]}\")\n",
    "\n",
    "# Apply softmax to get probabilities\n",
    "probabilities = torch.softmax(output, dim=1)\n",
    "print(f\"Sample probabilities: {probabilities[0]}\")\n",
    "print(f\"Predicted class: {torch.argmax(probabilities[0]).item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8663db62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_feature_maps_flow(model, input_tensor, max_channels=8):\n",
    "    \"\"\"\n",
    "    Visualize feature maps at different layers during forward pass\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Get feature maps from convolutional layers\n",
    "        feature_maps = model.get_feature_maps(input_tensor, layer_idx=3)\n",
    "    \n",
    "    # Take only the first sample from the batch\n",
    "    input_img = input_tensor[0, 0].cpu().numpy()\n",
    "    \n",
    "    n_layers = len(feature_maps)\n",
    "    fig, axes = plt.subplots(n_layers + 1, max_channels + 1, figsize=(20, 4 * (n_layers + 1)))\n",
    "    \n",
    "    # Show original input\n",
    "    axes[0, 0].imshow(input_img, cmap='gray')\n",
    "    axes[0, 0].set_title('Input Image')\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    # Hide unused subplots in first row\n",
    "    for j in range(1, max_channels + 1):\n",
    "        axes[0, j].axis('off')\n",
    "    \n",
    "    # Visualize feature maps for each convolutional layer\n",
    "    for layer_idx, feature_map in enumerate(feature_maps):\n",
    "        feature_map = feature_map[0].cpu().numpy()  # Take first sample\n",
    "        n_channels = min(feature_map.shape[0], max_channels)\n",
    "        \n",
    "        axes[layer_idx + 1, 0].text(0.5, 0.5, f'Conv Layer {layer_idx + 1}\\\\n{feature_map.shape[0]} channels\\\\n{feature_map.shape[1]}Ã—{feature_map.shape[2]} spatial', \n",
    "                                   ha='center', va='center', transform=axes[layer_idx + 1, 0].transAxes,\n",
    "                                   bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\"))\n",
    "        axes[layer_idx + 1, 0].axis('off')\n",
    "        \n",
    "        # Show individual feature maps\n",
    "        for channel_idx in range(n_channels):\n",
    "            feature = feature_map[channel_idx]\n",
    "            \n",
    "            im = axes[layer_idx + 1, channel_idx + 1].imshow(feature, cmap='viridis')\n",
    "            axes[layer_idx + 1, channel_idx + 1].set_title(f'Channel {channel_idx}')\n",
    "            axes[layer_idx + 1, channel_idx + 1].axis('off')\n",
    "            \n",
    "            # Add colorbar for reference\n",
    "            plt.colorbar(im, ax=axes[layer_idx + 1, channel_idx + 1], fraction=0.046, pad=0.04)\n",
    "        \n",
    "        # Hide unused subplots\n",
    "        for j in range(n_channels + 1, max_channels + 1):\n",
    "            axes[layer_idx + 1, j].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('Feature Maps Flow Through CNN Layers', fontsize=16, y=1.02)\n",
    "    \n",
    "    return feature_maps\n",
    "\n",
    "# Visualize feature maps flow\n",
    "print(\"Visualizing feature maps through the network:\")\n",
    "feature_maps = visualize_feature_maps_flow(model, sample_input[:1])  # Use only first sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b8f6ce",
   "metadata": {},
   "source": [
    "## 8. Backpropagation in CNNs\n",
    "\n",
    "Understanding how gradients flow backwards through convolutional layers is crucial. Let's implement and visualize the backpropagation process.\n",
    "\n",
    "### Mathematical Background\n",
    "\n",
    "For a convolutional layer, the gradient with respect to input $\\\\frac{\\\\partial L}{\\\\partial I}$ is computed by convolving the output gradient $\\\\frac{\\\\partial L}{\\\\partial O}$ with the **flipped** kernel:\n",
    "\n",
    "$$\\\\frac{\\\\partial L}{\\\\partial I}(i,j) = \\\\sum_{m=0}^{M-1} \\\\sum_{n=0}^{N-1} \\\\frac{\\\\partial L}{\\\\partial O}(i-m, j-n) \\\\cdot K(m,n)$$\n",
    "\n",
    "The gradient with respect to the kernel is:\n",
    "\n",
    "$$\\\\frac{\\\\partial L}{\\\\partial K}(m,n) = \\\\sum_{i,j} I(i+m, j+n) \\\\cdot \\\\frac{\\\\partial L}{\\\\partial O}(i,j)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e9032e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_gradients(model, input_tensor, target_class=0):\n",
    "    \"\"\"\n",
    "    Visualize gradients flowing through the network\n",
    "    \"\"\"\n",
    "    \n",
    "    # Enable gradient computation\n",
    "    model.train()  \n",
    "    input_tensor.requires_grad_(True)\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(input_tensor)\n",
    "    \n",
    "    # Create target (one-hot encoded)\n",
    "    target = torch.zeros_like(output)\n",
    "    target[0, target_class] = 1.0\n",
    "    \n",
    "    # Compute loss (we'll use simple MSE for demonstration)\n",
    "    loss = F.mse_loss(output, target)\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Collect gradients\n",
    "    input_grad = input_tensor.grad\n",
    "    \n",
    "    layer_grads = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None and 'weight' in name and 'conv' in name:\n",
    "            layer_grads.append((name, param.grad.clone()))\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    \n",
    "    # Input and its gradient\n",
    "    axes[0, 0].imshow(input_tensor[0, 0].detach().cpu().numpy(), cmap='gray')\n",
    "    axes[0, 0].set_title('Input Image')\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    input_grad_vis = input_grad[0, 0].cpu().numpy()\n",
    "    im1 = axes[1, 0].imshow(input_grad_vis, cmap='RdBu')\n",
    "    axes[1, 0].set_title('Input Gradient')\n",
    "    axes[1, 0].axis('off')\n",
    "    plt.colorbar(im1, ax=axes[1, 0], fraction=0.046, pad=0.04)\n",
    "    \n",
    "    # Visualize gradients of first few convolutional filters\n",
    "    for i, (name, grad) in enumerate(layer_grads[:3]):\n",
    "        if i < 3:\n",
    "            # Show a few filters from this layer\n",
    "            filter_grad = grad[0, 0].cpu().numpy()  # First output channel, first input channel\n",
    "            \n",
    "            axes[0, i+1].imshow(np.random.randn(5, 5), cmap='gray')  # Placeholder\n",
    "            axes[0, i+1].set_title(f'{name.split(\".\")[1]} Filter')\n",
    "            axes[0, i+1].axis('off')\n",
    "            \n",
    "            im = axes[1, i+1].imshow(filter_grad, cmap='RdBu')\n",
    "            axes[1, i+1].set_title(f'Filter Gradient')\n",
    "            axes[1, i+1].axis('off')\n",
    "            plt.colorbar(im, ax=axes[1, i+1], fraction=0.046, pad=0.04)\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(len(layer_grads)+1, 4):\n",
    "        axes[0, i].axis('off')\n",
    "        axes[1, i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f'Gradients for Target Class {target_class}', fontsize=14, y=1.02)\n",
    "    \n",
    "    print(f\"Loss: {loss.item():.4f}\")\n",
    "    print(f\"Input gradient magnitude: {input_grad.abs().mean().item():.6f}\")\n",
    "    \n",
    "    for name, grad in layer_grads:\n",
    "        print(f\"{name} gradient magnitude: {grad.abs().mean().item():.6f}\")\n",
    "    \n",
    "    return loss, input_grad, layer_grads\n",
    "\n",
    "# Visualize gradients\n",
    "print(\"Computing and visualizing gradients:\")\n",
    "loss, input_grad, layer_grads = visualize_gradients(model, sample_input[:1], target_class=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878cdd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_flow_analysis(model, input_tensor, num_classes=10):\n",
    "    \"\"\"\n",
    "    Analyze gradient magnitudes throughout the network for different target classes\n",
    "    \"\"\"\n",
    "    \n",
    "    gradient_magnitudes = {f'class_{i}': [] for i in range(num_classes)}\n",
    "    layer_names = []\n",
    "    \n",
    "    for target_class in range(num_classes):\n",
    "        model.zero_grad()\n",
    "        input_tensor.grad = None if input_tensor.grad is None else input_tensor.grad.zero_()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(input_tensor)\n",
    "        \n",
    "        # Create target\n",
    "        target = torch.zeros_like(output)\n",
    "        target[0, target_class] = 1.0\n",
    "        \n",
    "        # Compute loss and backward pass\n",
    "        loss = F.mse_loss(output, target)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Collect gradient magnitudes\n",
    "        class_gradients = []\n",
    "        current_layer_names = []\n",
    "        \n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None and 'weight' in name:\n",
    "                grad_magnitude = param.grad.abs().mean().item()\n",
    "                class_gradients.append(grad_magnitude)\n",
    "                if target_class == 0:  # Only collect names once\n",
    "                    current_layer_names.append(name.split('.')[1] + '.' + name.split('.')[2])\n",
    "        \n",
    "        gradient_magnitudes[f'class_{target_class}'] = class_gradients\n",
    "        if target_class == 0:\n",
    "            layer_names = current_layer_names\n",
    "    \n",
    "    # Plot gradient magnitudes\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Heatmap of gradient magnitudes across classes and layers\n",
    "    gradient_matrix = np.array([gradient_magnitudes[f'class_{i}'] for i in range(num_classes)])\n",
    "    \n",
    "    im = ax1.imshow(gradient_matrix, cmap='viridis', aspect='auto')\n",
    "    ax1.set_xlabel('Layer')\n",
    "    ax1.set_ylabel('Target Class')\n",
    "    ax1.set_title('Gradient Magnitudes Across Classes and Layers')\n",
    "    ax1.set_xticks(range(len(layer_names)))\n",
    "    ax1.set_xticklabels(layer_names, rotation=45)\n",
    "    ax1.set_yticks(range(num_classes))\n",
    "    ax1.set_yticklabels([f'Class {i}' for i in range(num_classes)])\n",
    "    plt.colorbar(im, ax=ax1)\n",
    "    \n",
    "    # Line plot showing gradient flow for each class\n",
    "    for i in range(min(num_classes, 5)):  # Show only first 5 classes for clarity\n",
    "        ax2.plot(gradient_magnitudes[f'class_{i}'], marker='o', label=f'Class {i}')\n",
    "    \n",
    "    ax2.set_xlabel('Layer Index')\n",
    "    ax2.set_ylabel('Gradient Magnitude')\n",
    "    ax2.set_title('Gradient Flow Through Network')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_xticks(range(len(layer_names)))\n",
    "    ax2.set_xticklabels(layer_names, rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return gradient_magnitudes, layer_names\n",
    "\n",
    "# Analyze gradient flow\n",
    "print(\"Analyzing gradient flow for different target classes:\")\n",
    "grad_magnitudes, layer_names = gradient_flow_analysis(model, sample_input[:1], num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781901f7",
   "metadata": {},
   "source": [
    "## 9. Training Loop with Real Data\n",
    "\n",
    "Now let's train our CNN on real data and visualize the learning process. We'll use the CIFAR-10 dataset and create comprehensive training visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f7056f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10 dataset\n",
    "def load_cifar10_data(batch_size=32, num_workers=2):\n",
    "    \"\"\"\n",
    "    Load and preprocess CIFAR-10 dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    # Data transformations\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    \n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    \n",
    "    # Load datasets\n",
    "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, \n",
    "                                           download=True, transform=transform_train)\n",
    "    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, \n",
    "                           num_workers=num_workers)\n",
    "    \n",
    "    testset = torchvision.datasets.CIFAR10(root='./data', train=False, \n",
    "                                          download=True, transform=transform_test)\n",
    "    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, \n",
    "                          num_workers=num_workers)\n",
    "    \n",
    "    # CIFAR-10 class names\n",
    "    classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "    \n",
    "    return trainloader, testloader, classes\n",
    "\n",
    "# Load the data with error handling\n",
    "print(\"Loading CIFAR-10 dataset...\")\n",
    "trainloader, testloader, class_names = safe_load_cifar10_data(batch_size=64, num_workers=0)\n",
    "\n",
    "# Visualize some samples\n",
    "def visualize_cifar10_samples(dataloader, classes, num_samples=16):\n",
    "    \"\"\"\n",
    "    Visualize random samples from CIFAR-10 (real or synthetic)\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Get a batch of training data\n",
    "        dataiter = iter(dataloader)\n",
    "        images, labels = next(dataiter)\n",
    "        \n",
    "        # Check if data is already normalized or synthetic\n",
    "        if images.min() < 0:  # Normalized data\n",
    "            # Denormalize for visualization\n",
    "            mean = torch.tensor([0.4914, 0.4822, 0.4465]).view(3, 1, 1)\n",
    "            std = torch.tensor([0.2023, 0.1994, 0.2010]).view(3, 1, 1)\n",
    "            images_denorm = images * std + mean\n",
    "            images_denorm = torch.clamp(images_denorm, 0, 1)\n",
    "        else:  # Synthetic data (already in [0,1] range)\n",
    "            images_denorm = torch.clamp(images, 0, 1)\n",
    "    \n",
    "    fig, axes = plt.subplots(4, 4, figsize=(12, 12))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i in range(min(num_samples, images.shape[0])):\n",
    "        img = images_denorm[i].permute(1, 2, 0).numpy()\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].set_title(f'{classes[labels[i]]}')\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(num_samples, 16):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "        plt.tight_layout()\n",
    "        plt.suptitle('CIFAR-10 Sample Images', fontsize=16, y=1.02)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  Error visualizing samples: {e}\")\n",
    "        print(\"Continuing with tutorial...\")\n",
    "\n",
    "# Try to visualize samples\n",
    "try:\n",
    "    visualize_cifar10_samples(trainloader, class_names)\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  Could not visualize samples: {e}\")\n",
    "    print(\"ðŸ“ Samples would be displayed here in normal operation\")\n",
    "\n",
    "print(f\"Training samples: {len(trainloader.dataset)}\")\n",
    "print(f\"Test samples: {len(testloader.dataset)}\")\n",
    "print(f\"Number of classes: {len(class_names)}\")\n",
    "print(f\"Classes: {class_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db5b37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model for CIFAR-10 (3 input channels instead of 1)\n",
    "cifar_model = ModernLeNet(num_classes=10, input_channels=3, dropout_rate=0.3)\n",
    "cifar_model = cifar_model.to(device)\n",
    "\n",
    "# Training setup\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cifar_model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "def train_epoch(model, trainloader, criterion, optimizer, device):\n",
    "    \"\"\"\n",
    "    Train for one epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    progress_bar = tqdm(trainloader, desc='Training')\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(progress_bar):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\n",
    "            'Loss': f'{running_loss/(batch_idx+1):.3f}',\n",
    "            'Acc': f'{100.*correct/total:.2f}%'\n",
    "        })\n",
    "    \n",
    "    return running_loss / len(trainloader), 100. * correct / total\n",
    "\n",
    "def test_epoch(model, testloader, criterion, device):\n",
    "    \"\"\"\n",
    "    Test for one epoch\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(testloader, desc='Testing'):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    return test_loss / len(testloader), 100. * correct / total\n",
    "\n",
    "# Training visualization setup\n",
    "class TrainingVisualizer:\n",
    "    def __init__(self):\n",
    "        self.train_losses = []\n",
    "        self.train_accs = []\n",
    "        self.test_losses = []\n",
    "        self.test_accs = []\n",
    "        self.epochs = []\n",
    "        \n",
    "        # Setup interactive plot\n",
    "        plt.ion()\n",
    "        self.fig, (self.ax1, self.ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    def update(self, epoch, train_loss, train_acc, test_loss, test_acc):\n",
    "        \"\"\"Update plots with new metrics\"\"\"\n",
    "        self.epochs.append(epoch)\n",
    "        self.train_losses.append(train_loss)\n",
    "        self.train_accs.append(train_acc)\n",
    "        self.test_losses.append(test_loss)\n",
    "        self.test_accs.append(test_acc)\n",
    "        \n",
    "        # Clear and plot\n",
    "        self.ax1.clear()\n",
    "        self.ax2.clear()\n",
    "        \n",
    "        # Loss plot\n",
    "        self.ax1.plot(self.epochs, self.train_losses, 'b-', label='Training Loss')\n",
    "        self.ax1.plot(self.epochs, self.test_losses, 'r-', label='Test Loss')\n",
    "        self.ax1.set_xlabel('Epoch')\n",
    "        self.ax1.set_ylabel('Loss')\n",
    "        self.ax1.set_title('Training and Test Loss')\n",
    "        self.ax1.legend()\n",
    "        self.ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Accuracy plot\n",
    "        self.ax2.plot(self.epochs, self.train_accs, 'b-', label='Training Accuracy')\n",
    "        self.ax2.plot(self.epochs, self.test_accs, 'r-', label='Test Accuracy')\n",
    "        self.ax2.set_xlabel('Epoch')\n",
    "        self.ax2.set_ylabel('Accuracy (%)')\n",
    "        self.ax2.set_title('Training and Test Accuracy')\n",
    "        self.ax2.legend()\n",
    "        self.ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.draw()\n",
    "        plt.pause(0.1)\n",
    "\n",
    "# Initialize visualizer\n",
    "visualizer = TrainingVisualizer()\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(f\"Model has {sum(p.numel() for p in cifar_model.parameters()):,} parameters\")\n",
    "\n",
    "# Training loop (reduced epochs for demo)\n",
    "num_epochs = 3\n",
    "best_acc = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'\\\\nEpoch {epoch+1}/{num_epochs}')\n",
    "    print('-' * 50)\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(cifar_model, trainloader, criterion, optimizer, device)\n",
    "    \n",
    "    # Test\n",
    "    test_loss, test_acc = test_epoch(cifar_model, testloader, criterion, device)\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Update visualization\n",
    "    visualizer.update(epoch+1, train_loss, train_acc, test_loss, test_acc)\n",
    "    \n",
    "    # Print epoch results\n",
    "    print(f'Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%')\n",
    "    print(f'Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.2f}%')\n",
    "    \n",
    "    # Save best model\n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "        torch.save(cifar_model.state_dict(), 'best_cifar_model.pth')\n",
    "        print(f'New best accuracy: {best_acc:.2f}%')\n",
    "\n",
    "plt.ioff()\n",
    "print(f'\\\\nTraining completed! Best test accuracy: {best_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b79f882",
   "metadata": {},
   "source": [
    "## 10. Visualizing Learned Filters\n",
    "\n",
    "After training, let's examine what filters our CNN has learned and how they've evolved from their initial random state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4193ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_learned_filters(model, layer_name='features.0', max_filters=16):\n",
    "    \"\"\"\n",
    "    Visualize the learned convolutional filters\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the specified layer\n",
    "    layer = None\n",
    "    for name, module in model.named_modules():\n",
    "        if name == layer_name and isinstance(module, nn.Conv2d):\n",
    "            layer = module\n",
    "            break\n",
    "    \n",
    "    if layer is None:\n",
    "        print(f\"Layer {layer_name} not found!\")\n",
    "        return\n",
    "    \n",
    "    # Get filters\n",
    "    filters = layer.weight.data.cpu().numpy()\n",
    "    num_filters = min(filters.shape[0], max_filters)\n",
    "    \n",
    "    # Calculate grid size\n",
    "    grid_size = int(np.ceil(np.sqrt(num_filters)))\n",
    "    \n",
    "    fig, axes = plt.subplots(grid_size, grid_size, figsize=(15, 15))\n",
    "    axes = axes.ravel() if grid_size > 1 else [axes]\n",
    "    \n",
    "    for i in range(num_filters):\n",
    "        filter_weights = filters[i]\n",
    "        \n",
    "        # For RGB filters, show all channels or the magnitude\n",
    "        if filter_weights.shape[0] == 3:  # RGB\n",
    "            # Show as RGB image\n",
    "            filter_img = np.transpose(filter_weights, (1, 2, 0))\n",
    "            # Normalize to [0, 1]\n",
    "            filter_img = (filter_img - filter_img.min()) / (filter_img.max() - filter_img.min() + 1e-8)\n",
    "            axes[i].imshow(filter_img)\n",
    "        else:\n",
    "            # Show first channel or grayscale\n",
    "            filter_img = filter_weights[0] if filter_weights.shape[0] > 1 else filter_weights\n",
    "            im = axes[i].imshow(filter_img, cmap='RdBu')\n",
    "            plt.colorbar(im, ax=axes[i], fraction=0.046, pad=0.04)\n",
    "        \n",
    "        axes[i].set_title(f'Filter {i}')\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(num_filters, grid_size * grid_size):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f'Learned Filters in {layer_name}', fontsize=16, y=1.02)\n",
    "    \n",
    "    # Print filter statistics\n",
    "    print(f\"Layer: {layer_name}\")\n",
    "    print(f\"Filter shape: {filters.shape}\")\n",
    "    print(f\"Weight range: [{filters.min():.4f}, {filters.max():.4f}]\")\n",
    "    print(f\"Weight std: {filters.std():.4f}\")\n",
    "\n",
    "def compare_filter_evolution(original_model, trained_model, layer_name='features.0'):\n",
    "    \"\"\"\n",
    "    Compare filters before and after training\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get filters from both models\n",
    "    original_filters = None\n",
    "    trained_filters = None\n",
    "    \n",
    "    for name, module in original_model.named_modules():\n",
    "        if name == layer_name and isinstance(module, nn.Conv2d):\n",
    "            original_filters = module.weight.data.cpu().numpy()\n",
    "            break\n",
    "    \n",
    "    for name, module in trained_model.named_modules():\n",
    "        if name == layer_name and isinstance(module, nn.Conv2d):\n",
    "            trained_filters = module.weight.data.cpu().numpy()\n",
    "            break\n",
    "    \n",
    "    if original_filters is None or trained_filters is None:\n",
    "        print(\"Could not find filters in one of the models!\")\n",
    "        return\n",
    "    \n",
    "    # Show first 8 filters\n",
    "    num_filters = min(8, original_filters.shape[0])\n",
    "    \n",
    "    fig, axes = plt.subplots(2, num_filters, figsize=(20, 6))\n",
    "    \n",
    "    for i in range(num_filters):\n",
    "        # Original filter\n",
    "        orig_filter = original_filters[i, 0] if original_filters.shape[1] > 1 else original_filters[i]\n",
    "        im1 = axes[0, i].imshow(orig_filter, cmap='RdBu', vmin=-0.5, vmax=0.5)\n",
    "        axes[0, i].set_title(f'Original Filter {i}')\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        # Trained filter  \n",
    "        trained_filter = trained_filters[i, 0] if trained_filters.shape[1] > 1 else trained_filters[i]\n",
    "        im2 = axes[1, i].imshow(trained_filter, cmap='RdBu', vmin=-0.5, vmax=0.5)\n",
    "        axes[1, i].set_title(f'Trained Filter {i}')\n",
    "        axes[1, i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('Filter Evolution: Before vs After Training', fontsize=16, y=1.02)\n",
    "    \n",
    "    # Calculate change statistics\n",
    "    change = np.abs(trained_filters - original_filters).mean()\n",
    "    print(f\"Average absolute change in filter weights: {change:.6f}\")\n",
    "\n",
    "# Create an untrained model for comparison\n",
    "untrained_model = ModernLeNet(num_classes=10, input_channels=3, dropout_rate=0.3)\n",
    "\n",
    "# Visualize learned filters from the trained model\n",
    "print(\"Visualizing learned filters from first convolutional layer:\")\n",
    "visualize_learned_filters(cifar_model, 'features.0', max_filters=16)\n",
    "\n",
    "print(\"\\\\nComparing filter evolution:\")\n",
    "compare_filter_evolution(untrained_model, cifar_model, 'features.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebae6044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_filter_responses(model, dataloader, class_names, layer_name='features.0'):\n",
    "    \"\"\"\n",
    "    Analyze how different filters respond to different classes\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Get the layer\n",
    "    layer = None\n",
    "    layer_idx = None\n",
    "    for idx, (name, module) in enumerate(model.named_modules()):\n",
    "        if name == layer_name and isinstance(module, nn.Conv2d):\n",
    "            layer = module\n",
    "            layer_idx = idx\n",
    "            break\n",
    "    \n",
    "    if layer is None:\n",
    "        print(f\"Layer {layer_name} not found!\")\n",
    "        return\n",
    "    \n",
    "    # Collect activations for each class\n",
    "    class_activations = {class_name: [] for class_name in class_names}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "            if batch_idx >= 10:  # Limit to first 10 batches for speed\n",
    "                break\n",
    "                \n",
    "            inputs = inputs.to(device)\n",
    "            \n",
    "            # Get feature maps from the specified layer\n",
    "            feature_maps = model.get_feature_maps(inputs, layer_idx=0)[0]  # First conv layer\n",
    "            \n",
    "            # Average pool over spatial dimensions to get per-filter activation\n",
    "            activations = feature_maps.mean(dim=[2, 3])  # Shape: [batch_size, num_filters]\n",
    "            \n",
    "            # Group by class\n",
    "            for i, target in enumerate(targets):\n",
    "                class_name = class_names[target.item()]\n",
    "                class_activations[class_name].append(activations[i].cpu().numpy())\n",
    "    \n",
    "    # Average activations per class\n",
    "    avg_activations = {}\n",
    "    for class_name in class_names:\n",
    "        if class_activations[class_name]:\n",
    "            avg_activations[class_name] = np.mean(class_activations[class_name], axis=0)\n",
    "        else:\n",
    "            avg_activations[class_name] = np.zeros(layer.out_channels)\n",
    "    \n",
    "    # Create activation heatmap\n",
    "    activation_matrix = np.array([avg_activations[class_name] for class_name in class_names])\n",
    "    \n",
    "    plt.figure(figsize=(15, 8))\n",
    "    im = plt.imshow(activation_matrix, cmap='viridis', aspect='auto')\n",
    "    plt.xlabel('Filter Index')\n",
    "    plt.ylabel('Class')\n",
    "    plt.title(f'Average Filter Activations by Class ({layer_name})')\n",
    "    plt.yticks(range(len(class_names)), class_names)\n",
    "    plt.colorbar(im, label='Average Activation')\n",
    "    \n",
    "    # Find most discriminative filters\n",
    "    activation_variance = np.var(activation_matrix, axis=0)\n",
    "    top_filters = np.argsort(activation_variance)[-5:][::-1]\n",
    "    \n",
    "    print(f\"Most discriminative filters (by variance): {top_filters}\")\n",
    "    \n",
    "    # Plot top discriminative filters\n",
    "    fig, axes = plt.subplots(1, len(top_filters), figsize=(15, 3))\n",
    "    filters = layer.weight.data.cpu().numpy()\n",
    "    \n",
    "    for i, filter_idx in enumerate(top_filters):\n",
    "        filter_weights = filters[filter_idx]\n",
    "        \n",
    "        if filter_weights.shape[0] == 3:  # RGB\n",
    "            filter_img = np.transpose(filter_weights, (1, 2, 0))\n",
    "            filter_img = (filter_img - filter_img.min()) / (filter_img.max() - filter_img.min() + 1e-8)\n",
    "            axes[i].imshow(filter_img)\n",
    "        else:\n",
    "            filter_img = filter_weights[0]\n",
    "            axes[i].imshow(filter_img, cmap='RdBu')\n",
    "        \n",
    "        axes[i].set_title(f'Filter {filter_idx}\\\\nVar: {activation_variance[filter_idx]:.3f}')\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('Most Discriminative Filters', fontsize=14, y=1.02)\n",
    "    \n",
    "    return avg_activations, top_filters\n",
    "\n",
    "print(\"Analyzing filter responses to different classes:\")\n",
    "avg_activations, top_filters = analyze_filter_responses(cifar_model, testloader, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ad91ce",
   "metadata": {},
   "source": [
    "## 11. Comparing Different CNN Architectures\n",
    "\n",
    "Let's implement and compare different CNN architectures to understand how design choices affect performance and behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d52e82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShallowCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A shallow CNN with only 2 convolutional layers\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ShallowCNN, self).__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(32 * 8 * 8, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        \n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "class DeepCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A deeper CNN with 5 convolutional layers\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(DeepCNN, self).__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            # Block 1\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            # Block 2\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            # Block 3\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128 * 4 * 4, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "def compare_architectures():\n",
    "    \"\"\"\n",
    "    Compare different CNN architectures\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create models\n",
    "    models = {\n",
    "        'Shallow CNN': ShallowCNN(num_classes=10),\n",
    "        'Modern LeNet': ModernLeNet(num_classes=10, input_channels=3),\n",
    "        'Deep CNN': DeepCNN(num_classes=10)\n",
    "    }\n",
    "    \n",
    "    # Compare model statistics\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    model_stats = {}\n",
    "    for name, model in models.items():\n",
    "        # Count parameters\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        \n",
    "        # Count layers\n",
    "        conv_layers = sum(1 for m in model.modules() if isinstance(m, nn.Conv2d))\n",
    "        linear_layers = sum(1 for m in model.modules() if isinstance(m, nn.Linear))\n",
    "        \n",
    "        # Calculate model size (MB)\n",
    "        param_size = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "        buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())\n",
    "        model_size = (param_size + buffer_size) / (1024 ** 2)\n",
    "        \n",
    "        model_stats[name] = {\n",
    "            'Total Parameters': total_params,\n",
    "            'Trainable Parameters': trainable_params,\n",
    "            'Conv Layers': conv_layers,\n",
    "            'Linear Layers': linear_layers,\n",
    "            'Model Size (MB)': model_size,\n",
    "            'Memory per Batch (MB)': 0  # Will be calculated during forward pass\n",
    "        }\n",
    "    \n",
    "    # Visualize comparisons\n",
    "    metrics = ['Total Parameters', 'Conv Layers', 'Model Size (MB)']\n",
    "    colors = ['skyblue', 'lightcoral', 'lightgreen']\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        values = [model_stats[name][metric] for name in models.keys()]\n",
    "        bars = axes[i].bar(models.keys(), values, color=colors[i], alpha=0.7)\n",
    "        axes[i].set_title(f'{metric} Comparison')\n",
    "        axes[i].set_ylabel(metric)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, value in zip(bars, values):\n",
    "            height = bar.get_height()\n",
    "            if metric == 'Total Parameters':\n",
    "                label = f'{value:,}'\n",
    "            elif metric == 'Model Size (MB)':\n",
    "                label = f'{value:.2f}'\n",
    "            else:\n",
    "                label = f'{value}'\n",
    "            axes[i].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                        label, ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Print detailed comparison\n",
    "    print(\"Detailed Architecture Comparison:\")\n",
    "    print(\"=\"*60)\n",
    "    for name, model in models.items():\n",
    "        stats = model_stats[name]\n",
    "        print(f\"\\\\n{name}:\")\n",
    "        print(f\"  Total Parameters: {stats['Total Parameters']:,}\")\n",
    "        print(f\"  Convolutional Layers: {stats['Conv Layers']}\")\n",
    "        print(f\"  Linear Layers: {stats['Linear Layers']}\")\n",
    "        print(f\"  Model Size: {stats['Model Size (MB)']:.2f} MB\")\n",
    "    \n",
    "    return models, model_stats\n",
    "\n",
    "# Compare architectures\n",
    "print(\"Comparing different CNN architectures:\")\n",
    "models_dict, stats_dict = compare_architectures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122ff8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_comparison_simulation():\n",
    "    \"\"\"\n",
    "    Simulate and compare training performance of different architectures\n",
    "    \"\"\"\n",
    "    \n",
    "    # Simulated training results (in practice, you would train each model)\n",
    "    # These are realistic values based on typical performance\n",
    "    simulated_results = {\n",
    "        'Shallow CNN': {\n",
    "            'epochs': list(range(1, 11)),\n",
    "            'train_acc': [45, 58, 65, 70, 72, 74, 75, 76, 76, 77],\n",
    "            'test_acc': [42, 55, 62, 67, 68, 69, 70, 70, 69, 68],\n",
    "            'train_loss': [1.8, 1.4, 1.2, 1.0, 0.9, 0.8, 0.75, 0.7, 0.68, 0.65],\n",
    "            'test_loss': [1.9, 1.5, 1.3, 1.1, 1.0, 0.95, 0.92, 0.90, 0.92, 0.95],\n",
    "            'training_time': 8.5  # minutes per epoch\n",
    "        },\n",
    "        'Modern LeNet': {\n",
    "            'epochs': list(range(1, 11)),\n",
    "            'train_acc': [52, 68, 75, 80, 83, 85, 87, 88, 89, 90],\n",
    "            'test_acc': [50, 65, 72, 76, 78, 80, 81, 82, 81, 80],\n",
    "            'train_loss': [1.6, 1.2, 0.9, 0.7, 0.6, 0.5, 0.45, 0.4, 0.38, 0.35],\n",
    "            'test_loss': [1.7, 1.3, 1.0, 0.8, 0.75, 0.7, 0.68, 0.66, 0.68, 0.70],\n",
    "            'training_time': 12.3\n",
    "        },\n",
    "        'Deep CNN': {\n",
    "            'epochs': list(range(1, 11)),\n",
    "            'train_acc': [48, 62, 72, 78, 82, 85, 88, 90, 92, 94],\n",
    "            'test_acc': [45, 58, 68, 73, 76, 78, 80, 82, 81, 79],\n",
    "            'train_loss': [1.7, 1.3, 1.0, 0.8, 0.65, 0.5, 0.4, 0.32, 0.25, 0.20],\n",
    "            'test_loss': [1.8, 1.4, 1.1, 0.9, 0.8, 0.75, 0.72, 0.70, 0.73, 0.78],\n",
    "            'training_time': 18.7\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Create comprehensive comparison plots\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    colors = {'Shallow CNN': 'blue', 'Modern LeNet': 'red', 'Deep CNN': 'green'}\n",
    "    \n",
    "    # Training accuracy\n",
    "    for model_name, results in simulated_results.items():\n",
    "        axes[0, 0].plot(results['epochs'], results['train_acc'], \n",
    "                       color=colors[model_name], marker='o', label=model_name)\n",
    "    axes[0, 0].set_title('Training Accuracy')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Accuracy (%)')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Test accuracy\n",
    "    for model_name, results in simulated_results.items():\n",
    "        axes[0, 1].plot(results['epochs'], results['test_acc'], \n",
    "                       color=colors[model_name], marker='s', label=model_name)\n",
    "    axes[0, 1].set_title('Test Accuracy')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Accuracy (%)')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Training loss\n",
    "    for model_name, results in simulated_results.items():\n",
    "        axes[0, 2].plot(results['epochs'], results['train_loss'], \n",
    "                       color=colors[model_name], marker='^', label=model_name)\n",
    "    axes[0, 2].set_title('Training Loss')\n",
    "    axes[0, 2].set_xlabel('Epoch')\n",
    "    axes[0, 2].set_ylabel('Loss')\n",
    "    axes[0, 2].legend()\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Test loss\n",
    "    for model_name, results in simulated_results.items():\n",
    "        axes[1, 0].plot(results['epochs'], results['test_loss'], \n",
    "                       color=colors[model_name], marker='d', label=model_name)\n",
    "    axes[1, 0].set_title('Test Loss')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Loss')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Training time comparison\n",
    "    model_names = list(simulated_results.keys())\n",
    "    training_times = [simulated_results[name]['training_time'] for name in model_names]\n",
    "    bars = axes[1, 1].bar(model_names, training_times, \n",
    "                         color=[colors[name] for name in model_names], alpha=0.7)\n",
    "    axes[1, 1].set_title('Training Time per Epoch')\n",
    "    axes[1, 1].set_ylabel('Time (minutes)')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, time in zip(bars, training_times):\n",
    "        height = bar.get_height()\n",
    "        axes[1, 1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                       f'{time:.1f}m', ha='center', va='bottom')\n",
    "    \n",
    "    # Overfitting analysis (train vs test accuracy gap)\n",
    "    for model_name, results in simulated_results.items():\n",
    "        gap = np.array(results['train_acc']) - np.array(results['test_acc'])\n",
    "        axes[1, 2].plot(results['epochs'], gap, \n",
    "                       color=colors[model_name], marker='x', label=model_name)\n",
    "    axes[1, 2].set_title('Overfitting Analysis (Train - Test Accuracy)')\n",
    "    axes[1, 2].set_xlabel('Epoch')\n",
    "    axes[1, 2].set_ylabel('Accuracy Gap (%)')\n",
    "    axes[1, 2].legend()\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "    axes[1, 2].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Summary analysis\n",
    "    print(\"Performance Analysis Summary:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for model_name, results in simulated_results.items():\n",
    "        best_test_acc = max(results['test_acc'])\n",
    "        final_gap = results['train_acc'][-1] - results['test_acc'][-1]\n",
    "        \n",
    "        print(f\"\\\\n{model_name}:\")\n",
    "        print(f\"  Best Test Accuracy: {best_test_acc:.1f}%\")\n",
    "        print(f\"  Final Overfitting Gap: {final_gap:.1f}%\")\n",
    "        print(f\"  Training Time/Epoch: {results['training_time']:.1f} min\")\n",
    "        print(f\"  Parameters: {stats_dict[model_name]['Total Parameters']:,}\")\n",
    "        \n",
    "        if final_gap > 10:\n",
    "            print(f\"  Status: High overfitting\")\n",
    "        elif final_gap > 5:\n",
    "            print(f\"  Status: Moderate overfitting\")\n",
    "        else:\n",
    "            print(f\"  Status: Good generalization\")\n",
    "\n",
    "# Run performance comparison\n",
    "print(\"Simulating performance comparison of different architectures:\")\n",
    "performance_comparison_simulation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4313d98",
   "metadata": {},
   "source": [
    "## Conclusion and Key Takeaways\n",
    "\n",
    "Congratulations! You've completed a comprehensive journey through Convolutional Neural Networks. Let's summarize the key concepts and insights:\n",
    "\n",
    "### ðŸ”‘ Key Concepts Learned\n",
    "\n",
    "1. **Convolution Operation**: The mathematical foundation of CNNs, where filters detect patterns through element-wise multiplication and summation.\n",
    "\n",
    "2. **Feature Hierarchies**: CNNs learn hierarchical representations - early layers detect edges and textures, deeper layers detect complex objects.\n",
    "\n",
    "3. **Parameter Sharing**: Convolutional filters are shared across spatial locations, making CNNs translation-invariant and parameter-efficient.\n",
    "\n",
    "4. **Pooling**: Reduces spatial dimensions while retaining important information, providing translation invariance and computational efficiency.\n",
    "\n",
    "5. **Backpropagation**: Gradients flow backwards through convolutions via cross-correlation with flipped kernels.\n",
    "\n",
    "### ðŸ“Š Architecture Design Principles\n",
    "\n",
    "- **Depth vs Width**: Deeper networks can learn more complex patterns but are harder to train and more prone to overfitting\n",
    "- **Filter Sizes**: Smaller filters (3Ã—3) are more efficient and can achieve the same receptive field as larger filters when stacked\n",
    "- **Regularization**: Batch normalization, dropout, and data augmentation help prevent overfitting\n",
    "- **Skip Connections**: Enable training of very deep networks (ResNet, DenseNet)\n",
    "\n",
    "### ðŸŽ¯ Practical Applications\n",
    "\n",
    "CNNs excel at:\n",
    "- **Image Classification**: Recognizing objects in images\n",
    "- **Object Detection**: Locating objects within images  \n",
    "- **Semantic Segmentation**: Pixel-level classification\n",
    "- **Medical Imaging**: Detecting anomalies in X-rays, MRIs\n",
    "- **Autonomous Driving**: Processing camera feeds for navigation\n",
    "\n",
    "### ðŸš€ Next Steps\n",
    "\n",
    "To deepen your understanding:\n",
    "1. Implement modern architectures (ResNet, VGG, EfficientNet)\n",
    "2. Explore transfer learning and pre-trained models\n",
    "3. Study object detection frameworks (YOLO, R-CNN)\n",
    "4. Learn about generative models (GANs, VAEs)\n",
    "5. Practice with different datasets and domains\n",
    "\n",
    "### ðŸ“š Mathematical Insights\n",
    "\n",
    "- Convolution in the spatial domain = multiplication in the frequency domain\n",
    "- CNN feature maps can be interpreted as learned basis functions\n",
    "- The universal approximation theorem applies to CNNs with sufficient depth and width\n",
    "- Gradient descent in high-dimensional parameter spaces has unique challenges and solutions\n",
    "\n",
    "Thank you for following this tutorial! The combination of mathematical rigor, visual intuition, and hands-on implementation should give you a solid foundation for working with CNNs in your data science projects."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
